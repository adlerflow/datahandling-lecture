---
title: 'Data Handling: Import, Cleaning and Visualisation'
subtitle: 'Lecture 10:<br>Exploratory Data Analysis and Visualization, Part II'
author: "Dr. Aur√©lien Sallin"
date: "2024-12-12"
echo: false
code-line-numbers: false
format:
  revealjs:
    template-partials:
       - ../../style/title-slide.html
    width: 1280
    height: 720
    smaller: true
    embed-resources: true
    self-contained: true
    slide-number: c
    theme: ../../style/hsgtheme.scss
highlight-style: dracula 
cache: false
execute:
  echo: true
---

```{r set-options, echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}
options(width = 100)
library(knitr)
library(dplyr)
library(tidyverse)
library(tidyr)
library(ggthemes)
```



# Recap: Data cleaning and Data Visualization

## Use the five building block from `dplyr()`

```{r, echo=FALSE, out.width = "90%", fig.align='center', fig.cap = 'Source: [Intro to R for Social Scientists](https://jaspertjaden.github.io/course-intro2r/week3.html)', purl=FALSE}
include_graphics("../../img/dplyr_blocks.png")
```


## Important additional tools

- `ifelse(test, yes, no)` returns a value with the same shape as the logical test. Filled with elements selected from either `yes` or `no` depending on whether the element of `test` is `TRUE` or `FALSE`
```{r, echo = TRUE, eval = FALSE}
df |> 
  mutate(gender = ifelse(gender == "m", 1, 0))
```
- `case_when` vectorises multiple `ifelse()` statements. It is the `dplyr` equivalent of `if...else`
```{r, echo = TRUE, eval = FALSE}
df |> 
  mutate(
    agegroup = case_when(
      age >= 0 & age < 18 ~ "0-18",
      age >= 18 & age < 64 ~ "19-64",
      age >= 65 & age < 100 ~ ">64",
      .default = "999"
    )
  )
```
- `stringr`: str_replace(), str_detect(), etc.
- `tolower` and `trimws`


## Data visualization through tables and graphs

#### A **chart** typically contains at least one axis, the **values are represented in terms of visual objects** (dots, lines, bars) and axes typically have scales or labels.

  - If we are interested in exploring, analyzing or communicating **patterns** in the data, charts are more useful than tables.
    
<br>

#### A **table** typically contains rows and columns, and the **values are represented by text**.

  - If we are interested in exploring, analyzing or communicating **specific numbers** in the data, tables are more useful than graphs.
  

## The grammar of graphics

- The *`ggplot2`* package is an implementation of Leland Wilkinson's 'Grammar of Graphics'.

- `ggplot2` is so good that it has become *THE* reference [In python, use `plotnine` to apply the grammar of graphics.]


## Grammar of graphics

```{r echo= FALSE,out.width="55%", fig.align="center"}
include_graphics("../../img/grammargraphics.jpg")
```


## The grammar of graphics in action

Example from [A Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-dimensional Data](https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149) using the built-in `mtcars` dataset in R.

```{r echo= TRUE, fig.align="center"}
mtcars # mtcars is a built-in dataset in R
```


## From two dimensions...
```{r echo= TRUE, fig.align="center"}
ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point() + 
  theme_bw()
```


## To three dimensions...

```{r echo= TRUE, fig.align="center"}
ggplot(mtcars, aes(x = wt, y = mpg, color=factor(gear))) + 
  geom_point() + 
  theme_bw()
```

## To four dimensions...

```{r echo= TRUE, fig.align="center"}
ggplot(mtcars, aes(x = wt, y = mpg, color=factor(gear), size = cyl)) + 
  geom_point() + 
  theme_bw()
```

## To four dimensions (with `facets`)...

```{r echo= TRUE, fig.align="center"}
ggplot(mtcars, aes(x = wt, y = mpg, color=factor(gear))) + 
  geom_point() +
  facet_wrap(~cyl) +
  theme_bw()
```

## To five dimensions
```{r echo= TRUE, fig.align="center"}
ggplot(mtcars, aes(x = wt, y = mpg, color=factor(gear), size = cyl)) + 
  geom_point() +
  facet_wrap(~am) +
  theme_bw()
```

## To six dimensions
```{r echo= TRUE, fig.align="center"}
ggplot(mtcars, aes(x = wt, y = mpg, color=factor(gear), size = cyl)) + 
  geom_point() +
  facet_grid(am ~ carb) +
  theme_bw()
```

<!-- ## To higher ? ... -->

<!-- ```{r echo= FALSE, fig.align="center"} -->
<!-- library(plotly) -->

<!-- # Create a dropdown menu to filter by carb -->
<!-- unique_am <- sort(unique(mtcars$am)) -->
<!-- dropdown_buttons <- lapply(unique_am, function(am_value) { -->
<!--   list( -->
<!--     method = "restyle", -->
<!--     args = list("transforms[0].value", am_value), -->
<!--     label = paste("Am:", am_value) -->
<!--   ) -->
<!-- }) -->

<!-- # Create the interactive plot -->
<!-- mtcars %>% -->
<!--   plot_ly( -->
<!--     x = ~wt,  -->
<!--     y = ~mpg,  -->
<!--     color = ~factor(gear),  -->
<!--     symbol = ~factor(cyl), -->
<!--     type = "scatter", -->
<!--     mode = "markers", -->
<!--     transforms = list( -->
<!--       list( -->
<!--         type = "filter", -->
<!--         target = ~am, -->
<!--         operation = "=", -->
<!--         value = unique_am[1] # Default to first carb value -->
<!--       ) -->
<!--     ), -->
<!--     marker = list(size = 10) # Set marker size constant -->
<!--   ) %>% -->
<!--   layout( -->
<!--     title = "Interactive Plot with Filter for 'am'", -->
<!--     xaxis = list(title = "Weight (wt)"), -->
<!--     yaxis = list(title = "Miles per Gallon (mpg)"), -->
<!--     updatemenus = list( -->
<!--       list( -->
<!--         buttons = dropdown_buttons, -->
<!--         direction = "down", -->
<!--         x = 1.1, -->
<!--         xanchor = "right", -->
<!--         y = 0.5, -->
<!--         yanchor = "top" -->
<!--       ) -->
<!--     ) -->
<!--   ) -->


<!-- ``` -->


## Quiz on `geom_bar()`

Which code produced the figure? (This question would not be an exam question, as it requires specific knowledge of `geom_bar()`. Solve it with R.)

```{r echo= TRUE, fig.align="center"}
got_data <- tibble(
  house = c("Stark", "Stark", "Stark", "Lannister", "Lannister", "Lannister",
            "Targaryen", "Targaryen", "Targaryen"),
  soldiers = c(5000, 3000, 500, 4000, 3800, 1900, 4200, 3500, 5000),
  year = c(1,2,3,1,2,3,1,2,3)
)
```

<br>

:::: {.columns}

::: {.column width="50%"}
```{r, echo = TRUE, eval = FALSE}
ggplot(got_data, aes(x = year, y = soldiers, fill = house)) +
  geom_bar(stat = "identity") +
  theme_classic()
```

```{r, echo = TRUE, eval = FALSE}
ggplot(got_data, aes(x = year, y = soldiers, fill = house)) +
  geom_histogram() +
  theme_classic()
```

```{r, echo = TRUE, eval = FALSE}
ggplot(got_data, aes(x = year, fill = house)) +
  geom_bar() +
  theme_classic()
```
:::

::: {.column width="50%"}

```{r echo= FALSE, fig.align="center"}
ggplot(got_data, aes(x = year, y = soldiers, fill = house)) +
  geom_bar(stat = "identity") +
  theme_classic()
```
:::

::::





# Today

## üì¢ Announcements: about the exam

#### Exam for exchange students
- üéÅ 19.12.2024 at 16:15 in room 01-207.

#### Lockdown browser
- Exam and LockDown Browser: check Sharepoint on [StudentWeb](https://universitaetstgallen.sharepoint.com/sites/PruefungenDE/SitePages/en/Digitale-Pr%C3%BCfungen-(BYOD).aspx#how-to-prepare-for-a-digital-exam) and test on Canvas. <br>**Password**: DataHandling2023



## üì¢ Announcements: about the exam

#### Expectations for the exam:

  - Same format as quizzes and mock exam, including True/False questions, multiple-choice, and multiple-correct options. These are designed to test your understanding of the material.
  - There will also be 3-4 essay-style questions aimed at evaluating your ability to apply your knowledge to new situations.
    
    - E.g., you might be asked to explain particular steps of the data analysis process in a given situation. You can use code, R concepts, or you can explain in plain English. The more precise the better.
  
  - **You will not be required to write exact R code**, but you should be able to interpret and understand the code provided in the exam. 
  - I expect you to be familiar with all R commands and concepts covered in the lectures, exercises, in-class code, and additional practice exercises.
  - The readings are not mandatory for the exam. The focus will be on the material discussed in class and during the exercises.



# Today


## Goals for today

#### Building on what we covered last week:

1. [Know how to conduct exploratory data analysis (EDA).]{style="color:#6b7280;"}
2. [Visualize data using tables.]{style="color:#6b7280;"}
3. Visualize data using the grammar of graphics.
4. Produce effective data visualization.

#### Today and next time (first hour)

5. Work with text data
6. Dashboard with Shiny




# From graphs to effective data visualization


## Data visualization: some principles

- Values are represented by their **position relative to the axes**: line charts and scatterplots.

- Values are represented by the **size of an area**:  bar charts and area charts. 

- Values are **continuous**: use chart type that visually connects elements (line chart).

- Values are **categorical**: use chart type that visually separates elements (bar chart).

(Source: [Data Visualization Basics for Economists](https://hhsievertsen.github.io/EconDataBook/data-visualization-basics.html))




## {class="inverse"}
‚ÄúGreatest number of ideas in the shortest time with the least ink in the smallest space‚Äù (Edward Tufte, 1983)


## Data visualization: some principles

:::: {style="display: flex;"}

::: {}
Recommendations from Edward Tufte's "The Visual Display of Quantitative Information" (1983)
:::

::: {}
```{r, echo=FALSE, out.width = "70%", fig.align='center',  purl=FALSE}
include_graphics("../../img/tufte.jpg")
```
:::

::::



## Lie Factor, or strive for graphical integrity 


:::: {.columns}

::: {.column width="50%"}
<br>

We can quantify the **Lie Factor** of a graph as a measure of how much the graphic distorts the data. 

"The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the quantities represented." (Tufte, 1983)

Lie Factor = $\frac{\text{size of effect shown in graphic}}{\text{size of effect in data}}$

:::

::: {.column width="50%"}
```{r, echo=FALSE, out.width = "60%", fig.align='center',  purl=FALSE}
include_graphics("../../img/doctors_tufte.jpg")
```
:::

::::


## Lie Factor, or strive for graphical integrity 

:::: {.columns}

::: {.column width="50%"}
<br>

Lie Factor = $\frac{\text{Yang had 39.1% of total ink}}{\text{Yang had 22.5%}}$ = 1.74
:::

::: {.column width="50%"}
```{r, echo=FALSE, out.width = "100%", fig.align='center',  purl=FALSE}
include_graphics("../../img/yangPOLL.jpg")
```
:::

<!-- Using a rule on the monitor: -->
<!-- Yang: 12.5*2 = 25 -->
<!-- Bernie: 8*1.5 = 12 -->
<!-- Buttigieg: 7*1.5 = 10.5 -->
<!-- Biden: 6*1.5 = 9 -->
<!-- Warren: 5*1.5 = 7.5 -->

<!-- In reality: 22.5/(22.5+21+17.9+11.5+9.6) = 27.27% -->
<!-- In the graph: 25/(25+12+10.5+9+7.5) = 39.1%, or 43.2% larger -->
::::

## Thou shalt not truncate the Y axis.

```{r, echo=FALSE, out.width = "50%", fig.align='center',  purl=FALSE}
include_graphics("../../img/carlson.png")
```


## Thou shalt not truncate the Y axis.

```{r echo= FALSE, out.width="120%", fig.align="center", fig.cap = "Source: [The lie factor and the baseline paradox](https://nightingaledvs.com/the-lie-factor-and-the-baseline-paradox/)" }
include_graphics("https://i0.wp.com/nightingaledvs.com/wp-content/uploads/2021/06/Hilje-6.png?resize=768%2C433&ssl=1")
```


## Avoid pie charts
```{r, echo=FALSE, out.width = "100%", fig.align='center',  purl=FALSE}
library(gridExtra)

# Create sample data
df1 <- data.frame(
  category = c("A", "B"),
  value = c(85, 15)
) 

df2 <- data.frame(
  category = c("A", "B"),
  value = c(55, 45)
) 

# Pie chart
pie_chart <- ggplot(df1, aes(x = "", y = value, fill = category)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y") +
  geom_text(aes(label = category),
            position = position_stack(vjust = 0.5),
            size = 7, color = "white") +
  labs(title = "Pie Chart") +
  theme_void() +
  scale_fill_brewer(palette = "Set1") +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    legend.position = "none"  # Remove the legend
  )

pie_chart2 <- ggplot(df2, aes(x = "", y = value, fill = category)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y") +
  geom_text(aes(label = category),
            position = position_stack(vjust = 0.5),
            size = 7, color = "white") +
  labs(title = "Pie Chart") +
  theme_void() +
  scale_fill_brewer(palette = "Set1") +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    legend.position = "none"  # Remove the legend
  )

# Combine the charts
grid.arrange(pie_chart, pie_chart2, ncol = 2)
```

"All variations lead to overestimation of small values and underestimation of large ones." [Kosara et al, 2018](https://diglib.eg.org/server/api/core/bitstreams/b0fc6407-96c8-4127-b1cb-6ca990b2ddb0/content)



## Different types of colors for different types of data

```{r echo=FALSE, eval = TRUE, out.width = "100%", message=FALSE, warning=FALSE, fig.align='center'}

n <- 40
data <- data.frame(
  x = 1:n,
  y = runif(n, min = 0, max = 5),
  group = rep(letters[1:5], each = n/5)
)

# Create a scatterplot
p <- ggplot(data, aes(x = x, y = y, color = group)) +
  geom_point(size = 3) +
  labs(
    x = "",
    y = "",
    color = ""
  ) +
  theme_classic() +
  theme(legend.position = "none")

p2 <- p + 
  labs(
    title = "Palette: Sequential",
    subtitle = 'scale_color_brewer(palette = "BuPu")'
  ) + 
  scale_color_brewer(palette = "BuPu")

p3 <- p + 
  labs(
    title = "Palette: Qualitative",
    subtitle = 'scale_color_brewer(palette = "Dark2")'
  ) + 
  scale_color_brewer(palette = "Dark2")

p4 <- p + 
  labs(
    title = "Palette: R Color brewer: Diverging",
    subtitle = 'scale_color_brewer(palette = "RdYlGn")'
  ) + 
  scale_color_brewer(palette = "RdYlGn")

library(harrypotter)
p1 <- p +
  labs(
    title = "Palette: Gryffindor üßô",
    subtitle = 'scale_color_hp(house = "Gryffindor", discrete = TRUE)'
  ) +
  scale_color_hp(house = "Gryffindor", discrete = TRUE)

grid.arrange(p2, p3, p4, p1, ncol = 2)

```




## Only what matters should be reported 

- Data-ink Ratio = $\frac{\text{ink used for data points}}{\text{total ink used to print the graphic
}}$

  - Data ink: data points and measured quantities, such as the dots in a scatter plot 
  - Non-data ink: functional marks such as titles, labels, axes, gridlines and tick points or decorative marks
  
Limits to this approach: we still need some ink to interpret and understand the data.


## Only what matters should be reported 

```{r echo=FALSE, out.width = "100%", message=FALSE, warning=FALSE, fig.align='center'}
library(gridExtra)
library(ggthemes)

# High data-ink ratio graph (plot1)
plot1 <- mtcars |> 
  group_by(cyl) |> 
  count() |> 
  ggplot(aes(x = as.factor(cyl), y = n, fill = as.factor(cyl), label = n)) +
  geom_bar(stat = "identity") +
  geom_label(color = "white", fontface = "bold", show.legend = FALSE) +
  labs(
    title = "Car Cylinder Count with High Data-Ink Ratio",
    subtitle = "Detailed representation with color, labels, and gridlines",
    x = "Number of Cylinders",
    y = "Count of Cars"
  ) +
  theme_minimal() +
  theme(
    panel.background = element_rect(fill = "lightblue", color = NA),
    panel.grid.major = element_line(color = "gray70", size = 0.5),
    panel.grid.minor = element_line(color = "gray85", size = 0.25),
    legend.position = "none",
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12, color = "gray20")
  )

# Minimalist graph (plot2)
plot2 <- mtcars |> 
  group_by(cyl) |> 
  count() |> 
  ggplot(aes(x = as.factor(cyl), y = n)) +
  geom_bar(stat = "identity", fill = "gray50") +
  labs(
    title = "Car Cylinder Count with Minimalist Design",
    x = "Number of Cylinders",
    y = "Count of Cars"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title = element_text(size = 10),
    axis.text = element_text(size = 8)
  )

# Arrange both plots side by side
grid.arrange(plot1, plot2, ncol = 2)
```
<details> <summary>Show code for the graphs</summary>
```{r echo=TRUE, eval = FALSE, out.width = "100%", message=FALSE, warning=FALSE, fig.align='center'}
library(gridExtra)
library(ggthemes)

# High data-ink ratio graph (plot1)
plot1 <- mtcars |> 
  group_by(cyl) |> 
  count() |> 
  ggplot(aes(x = as.factor(cyl), y = n, fill = as.factor(cyl), label = n)) +
  geom_bar(stat = "identity") +
  geom_label(color = "white", fontface = "bold", show.legend = FALSE) +
  labs(
    title = "Car Cylinder Count with High Data-Ink Ratio",
    subtitle = "Detailed representation with color, labels, and gridlines",
    x = "Number of Cylinders",
    y = "Count of Cars"
  ) +
  theme_minimal() +
  theme(
    panel.background = element_rect(fill = "lightblue", color = NA),
    panel.grid.major = element_line(color = "gray70", size = 0.5),
    panel.grid.minor = element_line(color = "gray85", size = 0.25),
    legend.position = "none",
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12, color = "gray20")
  )

# Minimalist graph (plot2)
plot2 <- mtcars |> 
  group_by(cyl) |> 
  count() |> 
  ggplot(aes(x = as.factor(cyl), y = n)) +
  geom_bar(stat = "identity", fill = "gray50") +
  labs(
    title = "Car Cylinder Count with Minimalist Design",
    x = "Number of Cylinders",
    y = "Count of Cars"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title = element_text(size = 10),
    axis.text = element_text(size = 8)
  )

# Arrange both plots side by side
grid.arrange(plot1, plot2, ncol = 2)
```
</details>


## Only what matters should be reported 

```{r echo= FALSE, out.width="120%", fig.align="center", fig.cap = "Source: [simplexct.com](https://simplexct.com/data-ink-ratio)" }
include_graphics("../../img/datainkration.png")
```


## Only what matters should be reported 

```{r echo= FALSE, out.width="120%", fig.align="center", fig.cap = "Source: [simplexct.com](https://simplexct.com/data-ink-ratio)" }
include_graphics("https://img.pagecloud.com/28E0b8Fkrd98Y1K8fxPD8bX0NqM=/918x0/filters:no_upscale()/simplexct/images/Fig2-b7d62.png")
```


## Only what matters should be reported 

Works for tables as well...

:::: {.columns}

::: {.column width="50%"}
```{r echo= FALSE, out.width="120%", fig.align="center", fig.cap = "Source: [simplexct.com](https://simplexct.com/data-ink-ratio-tables)"}
include_graphics("https://img.pagecloud.com/7xNdhRMc4inFClYsHQ5xxa4YQW4=/936x0/filters:no_upscale()/simplexct/images/Fig2-wd319.png")
```
:::

::: {.column width="50%"}
```{r echo= FALSE, out.width="120%", fig.align="center" }
include_graphics("https://img.pagecloud.com/6e1pFdr0cwYRNL4ehktNetsGuCI=/936x0/filters:no_upscale()/simplexct/images/Fig3-h15bd.png")
```
:::

::::

## Data density

The data density takes the number of data points that are being graphed relative to the physical size of the graphic to capture the principle of aiming to present many numbers in a small space: 

- Data density = $\frac{\text{number of entries in data matrix}}{\text{area of data graphic
}}$


## Data density

```{r echo=FALSE, eval = TRUE, out.width = "100%", message=FALSE, warning=FALSE, fig.align='center'}
two_by_two_matrix <- data.frame(candidate = c("Trump", "Harris"),
                                votes = c(49.8, 48.3))

```

:::: {.columns}

::: {.column width="50%"}

```{r echo=FALSE, eval = TRUE, out.width = "90%", message=FALSE, warning=FALSE, fig.align='center'}
ggplot(two_by_two_matrix, aes(x = candidate, y = votes)) +
  geom_col(fill = "skyblue") +
  geom_text(aes(label = votes), vjust = -0.5) +
  labs(subtitle = "Data Density Example",
       title = "Election results",
       x = "Candidate",
       y = "Votes") +
  theme_minimal()
```

:::

::: {.column width="30%"}

```{r echo=FALSE, eval = TRUE, out.width = "100%", message=FALSE, warning=FALSE, fig.align='center'}
library(gtExtras)
two_by_two_matrix |> 
  gt() |> 
  tab_header(
    title = md("Election results"),
    subtitle = md("Data Density Example")
  ) 
```
<!-- Give people the data so they can exercise their full powers ‚Äì don‚Äôt limit them. Give the control of the information to the viewer instead of the editor. -->
<!-- Data-thin displays move viewers towards ignorance and passivity. -->

:::

::: {.column width="20%"}
**49.8% voted for Trump, 48.3% for Harris.**
:::

::::

:::: {.columns}

::: {.column width="50%"}
(1) Graph
:::

::: {.column width="30%"}
(2) Table
:::

::: {.column width="20%"}
(3) Text
:::

::::



## Data visualization: from a graph to a story

- Two pieces of advice I personally received: 
    
  1. If possible, **fit your whole story in one graph**.
  2. Your audience should understand your graph **without the need of listening to you** or **reading your text.**

<br>

- Be simple and avoid unnecessary fanciness.
- Avoid pie charts and 3D charts. 



# A Design problem

## A Design Problem

What is wrong with the graph below? Create your own version of the graph.


```{r echo= FALSE, out.width="120%", fig.align="center", fig.cap = "Source: [perceptual edge](https://perceptualedge.com/example3.php)" }
include_graphics("../../img/example3problem.gif")
```


## A Design Problem

Use the following data to create your own version of the graph:

```{r, echo = TRUE}
dataChallenge <- data.frame(
  Location = rep(c("Bahamas Beach", "French Riviera", "Hawaiian Club"), each = 3),
  Fiscal_Year = rep(c("FY93", "FY94", "FY95"), times = 3),
  Revenue = c(
    250000, 275000, 350000,  # Bahamas Beach (FY93, FY94, FY95)
    260000, 200000, 210000,  # French Riviera (FY93, FY94, FY95)
    450000, 500000, 400000   # Hawaiian Club (FY93, FY94, FY95)
  )
)
```



## The problem with this graph

  - The 3-D bars are impossible to read.
  - The heavy grid lines offer nothing but distraction.
  - The vertically-oriented labels (i.e., the resort names and years) are difficult to read.
  - The years run from back to front, which is counter-intuitive.


## A first solution: comparative performance

:::: {.columns}

::: {.column width="60%"}
```{r echo=FALSE, out.width="85%", fig.width=4.9, fig.height=3.5, message=FALSE}
ggplot(dataChallenge, aes(x = Fiscal_Year, y = Revenue, fill = Location)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Resort Revenues by Location and Year",
    x = "",
    y = "Revenue (in USD)"
  ) +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete(labels = c("1993", "1994", "1995")) +
  theme_classic() +
  theme(legend.position = "bottom")
```
:::

::: {.column width="40%"}

- The three resorts have been arranged in order of rank, based on revenue, to highlight their comparative performance.
- The years have been arranged from left to right, which is intuitive.
- The legend has been placed below the bars. 

:::
::::
<details> <summary>Show code</summary>
```{r echo=TRUE, eval = FALSE, message=FALSE}
ggplot(dataChallenge, aes(x = Fiscal_Year, y = Revenue, fill = Location)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Resort Revenues by Location and Year",
    x = "Year",
    y = "Revenue (in USD)"
  ) +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete(labels = c("1993", "1994", "1995")) +
  theme_classic() +
  theme(legend.position = "bottom")
```
</details>


## A second solution: change of revenue over time

:::: {.columns}

::: {.column width="60%"}
```{r echo=FALSE, out.width="85%", fig.width=4.9, fig.height=3.5, message=FALSE}
ggplot(dataChallenge, aes(x = Fiscal_Year, y = Revenue, color = Location,
                          group = Location)) +
  geom_line() +
  labs(
    title = "Resort Revenues by Location and Year",
    x = "",
    y = "Revenue (in USD)"
  ) +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete(labels = c("1993", "1994", "1995"),
                   expand = expansion(add = c(0.5, 0.5))) +
  theme_classic() +
  theme(legend.position = "bottom")
```
:::

::: {.column width="40%"}

- This design makes it easy to see how revenue has changed from year to year at each of these resorts. 
- However, the magnitudes are difficult to read because the y-axis does not start at 0!
- The eye is still going back and forth between the lines and the legend.

:::
::::
<details> <summary>Show code</summary>
```{r echo=TRUE, eval = FALSE, message=FALSE}
ggplot(dataChallenge, aes(x = Fiscal_Year, y = Revenue, color = Location,
                          group = Location)) +
  geom_line() +
  labs(
    title = "Resort Revenues by Location and Year",
    x = "",
    y = "Revenue (in USD)"
  ) +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete(labels = c("1993", "1994", "1995"), 
                   expand = expansion(add = c(0.5, 0.5))) +
  theme_classic() +
  theme(legend.position = "bottom")
```
</details>


## A second solution: change of revenue over time

:::: {.columns}

::: {.column width="60%"}
```{r echo=FALSE, out.width="85%", fig.width=4.9, fig.height=3.5, message=FALSE}
ggplot(dataChallenge, aes(x = Fiscal_Year, y = Revenue, color = Location,
                          group = Location)) +
  geom_line(size = 2) +
  geom_text(data = dataChallenge[dataChallenge$Fiscal_Year == "FY95", ],
            aes(label = Location), hjust = 0, nudge_x = 0.1, nudge_y = 0) +
  labs(
    title = "Resort Revenues by Location and Year",
    x = "",
    y = "Revenue (in USD)"
  ) +
  scale_y_continuous(labels = scales::dollar, limits = c(0, 500000)) +
  scale_x_discrete(labels = c("1993", "1994", "1995"), 
                   expand = expansion(add = c(0.5, 1))) +
  theme_classic() +
  theme(legend.position = "none")  +
  scale_color_brewer(palette = "Set1")
```
:::

::: {.column width="40%"}

- This design makes it easy to see how revenue has changed from year to year at each of these resorts. 

:::
::::
<details> <summary>Show code</summary>
```{r echo=TRUE, eval = FALSE, message=FALSE}
ggplot(dataChallenge, aes(x = Fiscal_Year, y = Revenue, color = Location,
                          group = Location)) +
  geom_line(size = 2) +
  geom_text(data = dataChallenge[dataChallenge$Fiscal_Year == "FY95", ],
            aes(label = Location), hjust = 0, nudge_x = 0.1, nudge_y = 0) +
  labs(
    title = "Resort Revenues by Location and Year",
    x = "",
    y = "Revenue (in USD)"
  ) +
  scale_y_continuous(labels = scales::dollar, limits = c(0, 500000)) +
  scale_x_discrete(labels = c("1993", "1994", "1995"), 
                   expand = expansion(add = c(0.5, 1))) +
  theme_classic() +
  theme(legend.position = "none")  +
  scale_color_brewer(palette = "Set1")
```
</details>


## A second solution: change of revenue over time

:::: {.columns}

::: {.column width="60%"}
```{r echo=FALSE, out.width="85%", fig.width=4.9, fig.height=3.5, message=FALSE}
ggplot(dataChallenge, aes(x = Fiscal_Year, y = Revenue, color = Location,
                          group = Location)) +
  geom_line(size = 2) +
  geom_text(data = dataChallenge[dataChallenge$Fiscal_Year == "FY95", ],
            aes(label = Location), hjust = 0, nudge_x = 0.1, nudge_y = 0) +
  labs(
    title = "Resort Revenues by Location and Year",
    x = "",
    y = "Revenue (in USD)"
  ) +
  scale_y_continuous(labels = scales::dollar, limits = c(0, 500000)) +
  scale_x_discrete(labels = c("1993", "1994", "1995"), 
                   expand = expansion(add = c(0.5, 1))) +
  theme_classic() +
  theme(legend.position = "none") +
  scale_color_brewer(palette = "Greys")
```
:::

::: {.column width="40%"}

- Different color palette.

:::
::::
<details> <summary>Show code</summary>
```{r echo=FALSE, message=FALSE}
ggplot(dataChallenge, aes(x = Fiscal_Year, y = Revenue, color = Location,
                          group = Location)) +
  geom_line(size = 2) +
  geom_text(data = dataChallenge[dataChallenge$Fiscal_Year == "FY95", ],
            aes(label = Location), hjust = 0, nudge_x = 0.1, nudge_y = 0) +
  labs(
    title = "Resort Revenues by Location and Year",
    x = "",
    y = "Revenue (in USD)"
  ) +
  scale_y_continuous(labels = scales::dollar, limits = c(0, 500000)) +
  scale_x_discrete(labels = c("1993", "1994", "1995"), 
                   expand = expansion(add = c(0.5, 1))) +
  theme_classic() +
  theme(legend.position = "none") +
  scale_color_brewer(palette = "Greys")
```
</details>



## Conclusion

Data visualization is an art of story-telling, deception, and scientific exactitude ü§ì.



# Text data

## Text data is increasedly used
- Text as data has become increasingly available due to the Internet and text digitization.
- Examples: literary texts, financial analyses, social media reactions, political discourses, etc.
- Main challenge: **Text is unstructured**.



## Eight Steps in Text Analysis

Focus on steps 1-4 for this course.
```{r nlppipeline, echo=FALSE, out.width = "95%", fig.align='center',  purl=FALSE}
include_graphics("../../img/nlp_pipeline.jpg")
```

<!-- (1) Data acquisition: text data are collected from disparate sources. Examples are webpage scrapping, numerization of old administrative records, collection of tweets through an API, etc. (2) Text cleaning and (3) text preprocessing require the analyst to prepare the text in such a way that text information can be read by a statistical software. Most importantly, cleaning and preprocessing are important to remove all the "noise" from the text and get the relevant information. For instance, in many applications, punctuation and so-called "stopwords" are removed. At this step, text information is "transformed" into a matrix. (4) Feature engineering is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. The extraction of the relevant and meaningful features from the text must make sense and be informative in light of the context.  -->


## Key R Packages for Text Analysis

`tidytext`: Converts text to/from tidy formats. Works well with tidyverse.

`quanteda`: Comprehensive package for preprocessing, visualization, and statistical analysis.

<!-- steps (2) to (4) can be performed using the `quanteda` or the `tidytext` package. `tidytext` works very well with the packages of the `tidyverse` family, such as `dplyr` or `tidyr`. This package converts text to and from tidy formats.  -->

<!-- The package `quanteda` remains, however, the most complete and go-to package for text analysis in R. It allows R users to perform all the preprocessing steps very easily, and provides all the functions necessary for preprocessing, visualization, and even statistical analyses of the corpus -->

```{r packages, echo=FALSE}
pacman::p_load(
    tidytext,
    quanteda,
    readtext,
    stringr,
    quanteda.textstats,
    quanteda.textplots
)
```



## From Raw Text to Corpus

The base, raw material, of quantitative text analysis is a **corpus**. A corpus is, in NLP, *a collection of authentic text organized into datasets*.

- Example: Newspapers, novels, tweets, etc.
- In `quanteda`: A data frame with a character vector for documents and additional metadata columns.

<!-- The base, raw material, of quantitative text analysis is a **corpus**. A corpus is, in NLP, *a collection of authentic text organized into datasets*. A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets. Typically, the data structure of a corpus organizes the individual text documents (e.g., tweets) as individual components of a larger object (the entire corpus), whereby each document-component is linked to its corresponding metadata-component (containing data describing the characteristics of a document; e.g., the timestamp a tweet was posted, the author of the tweet, etc.). The following figure illustrates this data structure for a generic text corpus. -->

```{r corpus, echo=FALSE, out.width = "35%", fig.align='center',  purl=FALSE}
include_graphics("../../img/text_corpus.jpg")
```

<!-- In the specific case of `quanteda`, a corpus is a **a data frame consisting of a character vector for documents, and additional vectors for document-level variables**. In other words, a corpus is a data frame that contains, in each row, a text document, and additional columns with the corresponding metadata about the text. -->


## Parse text data

- Text in a raw form is often found in a `.json` format (after web scraping), in a `.csv` format, or in simple `.txt` files. 
- The first task is then to import the text data in R and transform it as a corpus. 
- We will use the `inauguration` corpus from `quanteda`, which is a standard corpus used in introductory text analysis. It contains the inauguration discourses of the five first US presidents. 
- This text data can be loaded from the `readtext` package. The text is contained in a csv file, and is loaded with the `read.csv()` function. The metadata of this corpus is the year of the inauguration and the name of the president taking office.

```{r inauguration}
# set path to the package folder
path_data <- system.file("extdata/", package = "readtext")

# import csv file
dat_inaug <- read.csv(paste0(path_data, "/csv/inaugCorpus.csv"))
names(dat_inaug)
```


## Create a corpus

```{r, echo = TRUE}
# Create a corpus
corp <- corpus(dat_inaug, text_field = "texts")
print(corp)

# Look at the metadata in the corpus using `docvars`
docvars(corp)

# In quanteda, the metadata in a corpus can be handled like data frames.
docvars(corp, field = "Century") <- floor(docvars(corp, field = "Year") / 100) + 1
```


## Regular Expressions

Used to detect patterns in strings, replace parts of text, extract information from text.

- The use of the `stringr()` package has made regular expressions easier to deal with.

  - `str_count()`
    
```{r regex, echo = TRUE}
# Count occurences of the word "peace"
str_count(corp, "[Pp]eace")

# Count occurences of the words "peace" OR "war"
str_count(corp, "[Pp]eace|[Ww]ar")
```


## Regular Expressions

Used to detect patterns in strings, replace parts of text, extract information from text.

- The use of the `stringr()` package has made regular expressions easier to deal with.

  - `str_count()`
    
```{r regex2, echo = TRUE}
# Count occurences of the mention of the first person pronoun "I"
str_count(corp, "I") # counts the number of "I" occurences. This is not what we want.
str_count(corp, "[I][[:space:]]") # counts the number of "I" followed by a space.

# Extract the first five words of each discourse
str_extract(corp, "^(\\S+\\s|[[:punct:]]|\\n){5}") # ^serves to anchor at the beginning of the string, (){5} shows the group of signs must be detected five times. \S if for any non-space character,  \s is for space, [[:punct:]] for punctuation, and \n for the string representation of paragraphs. Basically, it means: five the first five occurences of many non-space characters (+) followed either (|) by a space, a punctuation sign, or a paragraph sign.
```

## From Corpus to Tokens

**Tokens**: Building blocks of text (words, punctuation, etc.).

- LLMs operate on tokenized text as input. The tokenization process converts raw text into numerical representations that the model can process.

```{r tokens}
toks <- tokens(corp)
head(toks[[1]], 20)
```


## From Corpus to Tokens

**Tokens**: Building blocks of text (words, punctuation, etc.).

  - Remove punctuation and stopwords.
  - Create N-grams (e.g., "not friendly").
  
```{r tokens1}
# Remove punctuation
toks <- tokens(corp, remove_punct = TRUE)
head(toks[[1]], 20)

# Remove stopwords
stopwords("en")
toks <- tokens_remove(toks, pattern = stopwords("en"))
head(toks[[1]], 20)
```


## From Corpus to Tokens

```{r tokesn2, echo = TRUE}
# We can keep words we are interested in
tokens_select(toks, pattern = c("peace", "war", "great*", "unit*"))
```

## From Corpus to Tokens

```{r tokesn3, echo = TRUE}
# Remove "fellow" and "citizen"
toks <- tokens_remove(toks, pattern = c(
    "fellow*",
    "citizen*",
    "senate",
    "house",
    "representative*",
    "constitution"
))
```

## From Corpus to Tokens

```{r tokesn4, echo = TRUE}
# Build N-grams (onegrams, bigrams, and 3-grams)
toks_ngrams <- tokens_ngrams(toks, n = 2:3)

# Build N-grams based on a structure: keep n-grams that containt a "never"
toks_neg_bigram_select <- tokens_select(toks_ngrams, pattern = phrase("never_*"))
head(toks_neg_bigram_select[[1]], 30)
```


## From Tokens to Document-Term Matrix

  - **DTM**: Rows = documents, Columns = tokens.
  - Contains count frequencies or indicators.
  - Use domain knowledge to reduce DTM dimensions.

Code Example:
```{r dfm}
dfmat <- dfm(toks)
print(dfmat)
```

```{r dfm clean}
dfmat <- dfm(toks)
dfmat <- dfm_trim(dfmat, min_termfreq = 2) # remove tokens that appear less than 1 times
```


## Analyzing DTMs

Use DTMs for:

  - Machine learning models
  - Document classification
  - Predicting authorship



## Statistics 

Very basic statistics about documents are the **top features** of each document, the frequency of expressions in the corpus.

```{r dfm clean1}
library(quanteda.textstats)

tstat_freq <- textstat_frequency(dfmat, n = 5)

topfeatures(dfmat, 10)

```

## Statistics 

The frequency of tokens can be represented in a text plot.

```{r dfm clean2}
library(quanteda.textplots)
quanteda.textplots::textplot_wordcloud(dfmat, max_words = 100)
```

## Conclusion

  - **Tokens**: Absolutely crucial for LLMs. They determine how the model interprets text, manage context, and enable learning.
  - **DTMs**: Indirectly useful for preprocessing, exploratory analysis, or hybrid systems but less central to modern LLMs.
  
DTMs are still used in business cases for description of text input.